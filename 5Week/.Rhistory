# age
boxplot(data$age)$stats
data$age <- ifelse(data$age < 18 | data$age > 74, NA, data$age)
table(is.na(data$age))
data <- data %>% filter(!is.na(age))
# duration
boxplot(data$duration)$stats
data$duration <- ifelse(data$duration < 2 | data$duration > 1058, NA, data$duration)
table(is.na(data$duration))
data$age <- ifelse(is.na(data$age), 40.17, data$age)
# 데이터 확인 (남은 record는 5,755개 (11,162 -> 5,755 : 5,407개 record 삭제됨))
count(data)
str(data)
# 데이터 확인 (남은 record는 5,755개 (11,162 -> 5,755 : 5,407개 record 삭제됨))
count(data)
data <- data %>% filter(!is.na(duration))
# 데이터 확인 (남은 record는 5,755개 (11,162 -> 5,755 : 5,407개 record 삭제됨))
count(data)
str(data)
# 데이터 확인 (남은 record는 5,755개 (11,162 -> 5,755 : 5,407개 record 삭제됨))
count(data)
str(data)
# 데이터 확인 (남은 record는 5,755개 (11,162 -> 5,755 : 5,407개 record 삭제됨))
count(data)
data_yes <- filter(data, deposit == "yes")
data_no <- filter(data, deposit == "no")
describe(data_yes)
describe(data_no)
data
set.seed(10)
idx = createDataPartition(data$deposit, p = .7, list = F)
tr_set <- data[idx, ]
ts_set <- data[-idx, ]
# tr set 으로 decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_1 <- rpart(deposit~housing+marital+month+duration+previous,
data = tr_set
)
# ts set 으로 성능 평가
# 0.7711의 정확도 보임
pred = predict(fit_1, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
t
t
# ts set 으로 성능 평가
# 0.7711의 정확도 보임
pred = predict(fit_1, ts_set, type = "class",)
print(data.frame(ts_set, pred))
data <- read.csv (
file = "bank.csv",
header = TRUE
)
corData <- data
corData$deposit <- ifelse(corData$deposit == "yes", 1, corData$deposit)
corData$deposit <- ifelse(corData$deposit == "no", 0, corData$deposit)
data$deposit <- as.numeric(corData$deposit)
data <- tibble(age = data$age,
job = as.factor(data$job),
marital = as.factor(data$marital),
education = as.factor(data$education),
default = as.factor(data$default),
balance = data$balance,
housing = as.factor(data$housing),
loan = as.factor(data$loan),
contact = as.factor(data$contact),
day = as.factor(data$day),
month = as.factor(data$month),
duration = data$duration,
campaign = data$campaign,
pdays = data$pdays,
previous = data$previous,
poutcome = as.factor(data$poutcome),
deposit = as.factor(data$deposit))
data$job <- ifelse(data$job == 'unknown', NA, data$job)
table(is.na(data$job))
data <- data %>% filter(!is.na(job))
data$education <- ifelse(data$education == 'unknown', NA, data$education)
table(is.na(data$education))
data <- data %>% filter(!is.na(education))
data$contact <- ifelse(data$contact == 'unknown', NA, data$contact)
data$contact
table(is.na(data$contact))
data <- data %>% filter(!is.na(contact))
count(data)
# previous
boxplot(data$previous)$stats
data$previous <- ifelse(data$previous < 0 | data$previous > 2, NA, data$previous)
table(is.na(data$previous))
data <- data %>% filter(!is.na(previous))
# campaign
boxplot(data$campaign)$stats
data$campaign <- ifelse(data$campaign < 1 | data$campaign > 6, NA, data$campaign)
table(is.na(data$campaign))
data <- data %>% filter(!is.na(campaign))
# balance
boxplot(data$balance)$stats
data$balance <- ifelse(data$balance < -2282 | data$balance > 4256, NA, data$balance)
table(is.na(data$balance))
data <- data %>% filter(!is.na(balance))
# age
boxplot(data$age)$stats
data$age <- ifelse(data$age < 18 | data$age > 74, NA, data$age)
table(is.na(data$age))
data <- data %>% filter(!is.na(age))
# duration
boxplot(data$duration)$stats
data$duration <- ifelse(data$duration < 2 | data$duration > 1058, NA, data$duration)
table(is.na(data$duration))
data <- data %>% filter(!is.na(duration))
set.seed(10)
idx = createDataPartition(data$deposit, p = .7, list = F)
tr_set <- data[idx, ]
ts_set <- data[-idx, ]
# tr set 으로 decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_1 <- rpart(deposit~housing+marital+month+duration+previous,
data = tr_set
)
# ts set 으로 성능 평가
# 0.7711의 정확도 보임
pred = predict(fit_1, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# 예측변수를 더 최적화하기 위해서 하나씩 빼보고 더해가며 최적 예측변수 집합 구함 (정확도를 기준으로)
# 결과: marital 변수를 빼도 정확도가 동일하여 뺌.
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
# ts set 으로 성능 평가
# 0.7711 의 정확도 보임
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# 예측변수를 더 최적화하기 위해서 하나씩 빼보고 더해가며 최적 예측변수 집합 구함 (정확도를 기준으로)
# 결과: marital 변수를 빼도 정확도가 동일하여 뺌.
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
# tr set 으로 decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_1 <- rpart(deposit~housing+marital+month+duration+previous,
data = tr_set
)
#decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_1 <- rpart(deposit~housing+marital+month+duration+previous,
data = tr_set
)
pred = predict(fit_1, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# 예측변수를 더 최적화하기 위해서 하나씩 빼보고 더해가며 최적 예측변수 집합 구함 (정확도를 기준으로)
# 결과: marital 변수를 빼도 정확도가 동일하여 뺌.
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
# ts set 으로 성능 평가
# 0.7711 의 정확도 보임
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
confusionMatrix(ts_set$deposit, pred, positive = 1)
# ts set 으로 성능 평가
# 0.7711 의 정확도 보임
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 1)
data <- read.csv (
file = "bank.csv",
header = TRUE
)
corData <- data
corData$deposit <- ifelse(corData$deposit == "yes", 1, corData$deposit)
corData$deposit <- ifelse(corData$deposit == "no", 0, corData$deposit)
corData$deposit <- as.numeric(corData$deposit)
data <- tibble(age = data$age,
job = as.factor(data$job),
marital = as.factor(data$marital),
education = as.factor(data$education),
default = as.factor(data$default),
balance = data$balance,
housing = as.factor(data$housing),
loan = as.factor(data$loan),
contact = as.factor(data$contact),
day = as.factor(data$day),
month = as.factor(data$month),
duration = data$duration,
campaign = data$campaign,
pdays = data$pdays,
previous = data$previous,
poutcome = as.factor(data$poutcome),
deposit = as.factor(data$deposit))
data$job <- ifelse(data$job == 'unknown', NA, data$job)
table(is.na(data$job))
data <- data %>% filter(!is.na(job))
data$education <- ifelse(data$education == 'unknown', NA, data$education)
table(is.na(data$education))
data <- data %>% filter(!is.na(education))
data$contact <- ifelse(data$contact == 'unknown', NA, data$contact)
data$contact
table(is.na(data$contact))
data <- data %>% filter(!is.na(contact))
count(data)
# previous
boxplot(data$previous)$stats
data$previous <- ifelse(data$previous < 0 | data$previous > 2, NA, data$previous)
table(is.na(data$previous))
data <- data %>% filter(!is.na(previous))
# campaign
boxplot(data$campaign)$stats
data$campaign <- ifelse(data$campaign < 1 | data$campaign > 6, NA, data$campaign)
table(is.na(data$campaign))
data <- data %>% filter(!is.na(campaign))
# balance
boxplot(data$balance)$stats
data$balance <- ifelse(data$balance < -2282 | data$balance > 4256, NA, data$balance)
table(is.na(data$balance))
data <- data %>% filter(!is.na(balance))
# age
boxplot(data$age)$stats
data$age <- ifelse(data$age < 18 | data$age > 74, NA, data$age)
table(is.na(data$age))
data <- data %>% filter(!is.na(age))
# duration
boxplot(data$duration)$stats
data$duration <- ifelse(data$duration < 2 | data$duration > 1058, NA, data$duration)
table(is.na(data$duration))
data <- data %>% filter(!is.na(duration))
set.seed(10)
idx = createDataPartition(data$deposit, p = .7, list = F)
tr_set <- data[idx, ]
ts_set <- data[-idx, ]
#decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_1 <- rpart(deposit~housing+marital+month+duration+previous,
data = tr_set
)
pred = predict(fit_1, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# 예측변수를 더 최적화하기 위해서 하나씩 빼보고 더해가며 최적 예측변수 집합 구함 (정확도를 기준으로)
# 결과: marital 변수를 빼도 정확도가 동일하여 뺌.
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
# ts set 으로 성능 평가
# 0.7711 의 정확도 보임
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
#decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
#decision tree 모델 학습 (위에서 선정한 예측변수 대입)
fit_2 <- rpart(deposit~housing+month+duration+previous,
data = tr_set
)
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
pred = predict(fit_2, tr_set, type = "class",)
print(data.frame(tr_set, pred))
confusionMatrix(tr_set$deposit, pred, positive = 'yes')
pred = predict(fit_2, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
system.time(rpartFit1 <- train(deposit~housing+month+duration+previous,
data = tr_set,
method = "rpart",
tuneLength = 10,
trControl = train.control,
metric = "ROC"))
rpartFit1
plot(rpartFit1)
fit_3 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp =0.002
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임
pred = predict(fit_3, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# maxdepth 최적화
# caret 패키지 사용 (rpart2 를 이용하면 maxdepth 파라미터 최적화 가능)
modelLookup("rpart2")
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
system.time(rpartFit2 <- train(deposit~housing+month+duration+previous,
data = tr_set,
method = "rpart2",
tuneLength = 10,
trControl = train.control,
metric = "ROC"))
rpartFit2
plot(rpartFit2)
# i 와 dataframe을 인자로 받아 인자로 받은i와 minsplit을 i로 하여 나온 정확도를 벡터로 생성하여
# 인자로받은 dataframe에 결합하여 반환해주는 함수 정의 (optMinSplit())
optMinSplit <- function(i, df) {
fit_o <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.004,
maxdepth = 8,
minsplit = i
)
pred = predict(fit_o, ts_set, type = "class",)
print(i)
print(confusionMatrix(pred, ts_set$deposit)[[3]][1])
df <- rbind(df, c(i, confusionMatrix(pred, ts_set$deposit)[[3]][1]))
return(df)
}
# 빈 dataframe 생성
df_optMinSplit <- data.frame()
# for loop을 이용하여 minsplit 값을 1부터 100까지 넣어가며 결과 확인
for(i in 1:100) {
df_optMinSplit <- optMinSplit(i, df_optMinSplit)
}
# 그래프로 시각화 (x -> 최적화 파라미터  //  y -> accuracy)
colnames(df_optMinSplit) = c('minsplit', 'Accuracy')
H_optMinSplit <- ggplot(df_optMinSplit, aes(x = minsplit , y = Accuracy)) +
theme(legend.position="top") + geom_line()
H_optMinSplit
fit_5 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 49
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임
pred = predict(fit_5, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
fit_5 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 52
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임
pred = predict(fit_5, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
fit_5 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 49
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임
pred = predict(fit_5, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
fit_3 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp =0.002
)
#8001
pred = predict(fit_3, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
fit_5 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 49
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임
pred = predict(fit_5, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
optMinBucket <- function(i, df) {
fit_o <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.004,
maxdepth = 8,
minsplit = 64,
minbucket = i
)
pred = predict(fit_o, ts_set, type = "class",)
print(i)
print(confusionMatrix(pred, ts_set$deposit)[[3]][1])
df <- rbind(df, c(i, confusionMatrix(pred, ts_set$deposit)[[3]][1]))
return(df)
}
# 빈 dataframe 생성
df_optMinBucket <- data.frame()
# for loop을 이용하여 minbucket 값을 1부터 100까지 넣어가며 결과 확인
for(i in 1:100) {
df_optMinBucket <- optMinBucket(i, df_optMinBucket)
}
# 그래프로 시각화 (x -> 최적화 파라미터  //  y -> accuracy)
colnames(df_optMinBucket) = c('minbucket', 'Accuracy')
H_optMinBucket <- ggplot(df_optMinBucket, aes(x = minbucket , y = Accuracy)) +
theme(legend.position="top") + geom_line()
H_optMinBucket
fit_6 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 49,
minbucket = 16
)
# ts set 으로 성능 평가
# 0.8094 의 정확도 보임 0.7786
pred = predict(fit_6, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
# split criterion 최적화 ('gini' or 'information')
# split criterion 의 default 값은 'gini' 이므로 'information'만 확인
fit_info <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
parms = list(split = 'information'),
cp = 0.002,
maxdepth = 8,
minsplit = 49,
minbucket = 16
)
# ts set 으로 성능 평가 0.7786
# 0.8001 의 정확도 보임 ==> split criterion 이 'gini' 인 모델보다 정확도 0.0083 하락
# 따라서 split criterion 의 최적값은 'gini' (default 값)
pred = predict(fit_info, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
opt_tr_set <- function(model, df) {
for(i in 6:9) {
percent = i*0.1
mean_acc = 0
for(k in 1:5) {
set.seed(k+1)
idx_o = createDataPartition(data$deposit, p = percent, list = F)
tr_set_o <- data[idx_o, ]
ts_set_o <- data[-idx_o, ]
pred = predict(model, ts_set_o, type = "class",)
confusionMatrix(ts_set_o$deposit, pred, positive = 'yes')
mean_acc = mean_acc + confusionMatrix(pred, ts_set_o$deposit)[[3]][1]
}
mean_acc = mean_acc / 5
df <- rbind(df, c(percent, mean_acc))
}
return(df)
}
# model 을 사전가지치기 모델로 하여 trainingset 비율 최적화
df_optSet_pre <- data.frame()
df_optSet_pre <- opt_tr_set(fit_6, df_optSet_pre)
df_optSet_pre
# 그래프로 시각화 (x -> 최적화 파라미터  //  y -> accuracy)
colnames(df_optSet_pre) = c('tr_set_percent', 'Accuracy')
H_optSet_pre <- ggplot(df_optSet_pre, aes(x = tr_set_percent , y = Accuracy)) +
theme(legend.position="top") + geom_line()
H_optSet_pre
# 사전 가지치기 모델 시각화
windows()
fancyRpartPlot(fit_pre)
readline('Enter to resume ')
# minsplite 최적화 -> 49
# minbucket 최적화 -> 16
# gini
fit_6 <- rpart(deposit~housing+month+duration+previous,
data = tr_set,
cp = 0.002,
maxdepth = 8,
minsplit = 49,
minbucket = 16
)
#7995
pred = predict(fit_6, ts_set, type = "class",)
print(data.frame(ts_set, pred))
confusionMatrix(ts_set$deposit, pred, positive = 'yes')
library(tidyverse)
library(rpart)
library(e1071)
library(rpart)
library(e1071)
library(caret)
# library(xlsx)
library(nnet)
set.seed(100) # Reproducibility setting
getModelInfo()
modelLookup("nnet")
# read the data
getwd()
# setwd("L:/내 드라이브/Rmarkdown")
fishing<- read.csv("Data/Fishing.csv")
# setwd("L:/내 드라이브/Rmarkdown")
fishing<- read.csv("Fishing.csv")
# setwd("L:/내 드라이브/Rmarkdown")
fishing<- read.csv("Fishing.csv")
# Source code listing.
library(tidyverse)
library(rpart)
library(e1071) # svmLinear
library(caret)
library(kernlab) # svmLinear2
set.seed(100) # Reproducibility setting
names(getModelInfo("svm"))
modelLookup("svmLinear2")
# read the data
getwd()
# setwd("L:/내 드라이브/Rmarkdown")
fishing<- read.csv("Fishing.csv")
